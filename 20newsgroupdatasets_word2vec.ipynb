{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "Zx0KGIWMh5tX",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "from gensim.models import word2vec\n",
    "\n",
    "corpus = []\n",
    "\n",
    "with codecs.open(\"/content/drive/My Drive/all_maeshori_train_utf8.txt\", \"r\", \"utf-8\") as f:\n",
    "    text = f.read().splitlines()\n",
    "for sen in text:\n",
    "  temp = sen.split()\n",
    "  corpus.append(temp[1:])\n",
    "\n",
    "#モデルを作る\n",
    "#sg=1ならskip-gram\n",
    "#size:単語ベクトルの次元数\n",
    "#window:ウインドウサイズ5\n",
    "#negative:ネガティブサンプリング数\n",
    "#alpha:学習率\n",
    "#min_count:設定した頻度　デフォルトだと頻度が5未満の文字は無視されるようだ\n",
    "#batch_words:学習時のバッチサイズ\n",
    "print(\"learn\")\n",
    "model = word2vec.Word2Vec(corpus, sg=1, size=200, window=5,negative=10,alpha=0.005,batch_words=10,min_count=1)\n",
    "print(\"save\")\n",
    "model.save('/content/drive/My Drive/all_maeshori_train_word2vec2_utf8.model')\n",
    "print(\"save done\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "Ndh7PBC266RL",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "from gensim.models import word2vec\n",
    "\n",
    "model = word2vec.Word2Vec.load('/content/drive/My Drive/all_maeshori_train_word2vec2_utf8.model')\n",
    "\n",
    "word = \"bony\"\n",
    "\n",
    "# 単語のベクトルを見る\n",
    "try:\n",
    "  word_vector = model.wv[word]\n",
    "  print(word_vector)\n",
    "except KeyError as error:\n",
    "  print(error)\n",
    "\n",
    "# 尾張と類似している単語を見る\n",
    "#similar_words = model.wv.most_similar(positive=[word], topn=9)\n",
    "#print(*[\" \".join([v, str(\"{:.2f}\".format(s))]) for v, s in similar_words], sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "Odu43EM3WjPv",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "from gensim.models import word2vec\n",
    "import numpy as np\n",
    "import codecs\n",
    "\n",
    "model = word2vec.Word2Vec.load('/content/drive/My Drive/all_maeshori_train_word2vec2_utf8.model')\n",
    "\n",
    "def avg_feature_vector(sentence, model, num_features):\n",
    "    words = sentence.split()\n",
    "    feature_vec = np.zeros(num_features, dtype=\"float\") # 特徴ベクトルの入れ物を初期化\n",
    "    for word in words:\n",
    "        feature_vec = np.add(feature_vec, model[word]) #行列の足し算\n",
    "    if len(words) > 0:\n",
    "        feature_vec = np.divide(feature_vec, len(words)) #文章の単語数で割り算\n",
    "    return feature_vec\n",
    "\n",
    "#print(avg_feature_vector(\"subject make ghostscript work organization lehigh university lines need font files get also advice get cica nice interface ghostscript ghostscript user interface makes user friendly using interface get copy windows directory copy gui executables files ghostscript directory line set ghostscript gui directory ready use enjoy steve brewer files need download ghostscript never used ghostscript files downloaded cica unfortunately seem work needs files want run ghostscript windows pc understand versions environments files need download get info steve brewer\", model, 200))\n",
    "#print(model.wv['subject'])\n",
    "#print(model.wv['make'])\n",
    "#sentence = \"subject make ghostscript work organization lehigh university lines need font files get also advice get cica nice interface ghostscript ghostscript user interface makes user friendly using interface get copy windows directory copy gui executables files ghostscript directory line set ghostscript gui directory ready use enjoy steve brewer files need download ghostscript never used ghostscript files downloaded cica unfortunately seem work needs files want run ghostscript windows pc understand versions environments files need download get info steve brewer\"\n",
    "\n",
    "doc_vec = []\n",
    "label = []\n",
    "\n",
    "with codecs.open(\"/content/drive/My Drive/all_maeshori_train_utf8.txt\", \"r\", \"utf-8\") as f:\n",
    "    text = f.read().splitlines()\n",
    "for s in text:\n",
    "  s = s.split()\n",
    "  cat = s[0]\n",
    "  sen = s[1:]\n",
    "  sen = ' '.join(sen)\n",
    "  doc_vec.append(avg_feature_vector(sen, model, 200))\n",
    "  label.append(cat)\n",
    "\n",
    "\n",
    "#print(doc_vec)\n",
    "#print(label)\n",
    "\n",
    "np.save('/content/drive/My Drive/np_save',doc_vec)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "qtusVc-ZrFBx",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.externals import joblib\n",
    "import codecs\n",
    "\n",
    "# コストパラメータやRBFカーネルパラメータはgrid_searchによって最適なパラメータを選択\n",
    "svm = svm.SVC(C = 10, gamma = 0.1, kernel = 'rbf')\n",
    "# train_docはベクトル化した文書のリスト\n",
    "# labelsは文書のカテゴリのリスト\n",
    "\n",
    "train_doc = np.load('/content/drive/My Drive/np_save.npy')\n",
    "labels = []\n",
    "\n",
    "with codecs.open(\"/content/drive/My Drive/all_maeshori_train_utf8.txt\", \"r\", \"utf-8\") as f:\n",
    "    text = f.read().splitlines()\n",
    "for s in text:\n",
    "  s = s.split()\n",
    "  cat = s[0]\n",
    "  labels.append(cat)\n",
    "\n",
    "svm.fit(train_doc, labels)\n",
    "#分類器の保存\n",
    "joblib.dump(svm, '/content/drive/My Drive/svm.pkl.cmp', compress = True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "mNnPLjXypFW0",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "a = np.load('/content/drive/My Drive/np_save.npy')\n",
    "print(type(a))\n",
    "print(a.shape)\n",
    "\n",
    "print(a)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "20newsgroupdatasets_word2vec",
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
